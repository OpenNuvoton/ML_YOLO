{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079d614b-d1ce-4f24-b18d-5b61eff95307",
   "metadata": {},
   "source": [
    "# Prepare COCO data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea0f6e-5456-4e3e-830d-b1357a22065a",
   "metadata": {},
   "source": [
    "## 1. check the downloaded dataset broken or not\n",
    "- This will delete the broken plots, and please redownload the broken plots again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e3de1d-305e-4c2d-98eb-8588ce24eea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0d9367d3f4486d881f6dde63fc8f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Check the broken plots:   0%|          | 0/349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the dataset's plots broken or not\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def __load_images_from_folder(folder):  ### 使用yield，降低ram使用\n",
    "        count = 0\n",
    "        for filename in os.listdir(folder):\n",
    "            count += 1\n",
    "            yield filename\n",
    "def __check_dataset_nums(folder):\n",
    "    return len(os.listdir(folder))\n",
    "\n",
    "def is_valid(file):\n",
    "    valid = True\n",
    "    try:\n",
    "        Image.open(file).load()\n",
    "    except OSError:\n",
    "        valid = False\n",
    "    return valid\n",
    "\n",
    "def check_dataset_plot(dataset_loc):\n",
    "    total_size = __check_dataset_nums(dataset_loc)\n",
    "    with tqdm(total=total_size, desc='Check the broken plots') as pbar:\n",
    "        for img_filename in __load_images_from_folder(dataset_loc):\n",
    "            img_filename = os.path.join(dataset_loc, img_filename)\n",
    "            \n",
    "            if not is_valid(img_filename):\n",
    "                print(\"Broken and delete it: {}\".format(img_filename))\n",
    "                os.remove(img_filename)\n",
    "            pbar.update(1)    \n",
    "\n",
    "# Update to the path of your own image dataset            \n",
    "#check_dataset_plot(r\"C:\\Users\\USER\\Desktop\\ML\\ML_tf2_object_detection_nu\\image_dataset\\coco-2017-50-2class\\train\\data\")\n",
    "check_dataset_plot(r\"C:\\Users\\USER\\Desktop\\ML\\ML_tf2_object_detection_nu\\image_dataset\\coco-2017-50-2class\\validation\\data\")\n",
    "#check_dataset_plot(r\"C:\\Users\\USER\\Desktop\\ML_tf2_object_detection_nu\\image_dataset\\coco-2017-person\\validation\\data\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcce610-e309-43e9-ba10-059931395287",
   "metadata": {},
   "source": [
    "## 2. Convert json to txt (YOLO format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9488a90f-1098-4392-a781-5bb74b2d7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191426f1-81ad-4e00-85d3-595d9a95ea74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 18] ['cat', 'dog']\n",
      "Create the yolo annotation files in: C:\\Users\\USER\\Desktop\\Yolo-Fastest-darknet\\training_demo\\output_yolo_dogcat\\train\n",
      "Create the dataset path file: C:\\Users\\USER\\Desktop\\Yolo-Fastest-darknet\\training_demo\\output_yolo_dogcat\\train.txt\n",
      "Create the classes file: C:\\Users\\USER\\Desktop\\Yolo-Fastest-darknet\\training_demo\\output_yolo_dogcat\\coco.names\n",
      "[WinError 183] 當檔案已存在時，無法建立該檔案。: 'C:\\\\Users\\\\USER\\\\Desktop\\\\Yolo-Fastest-darknet\\\\training_demo\\\\output_yolo_dogcat'\n",
      "skip create\n",
      "[17, 18] ['cat', 'dog']\n",
      "Create the yolo annotation files in: C:\\Users\\USER\\Desktop\\Yolo-Fastest-darknet\\training_demo\\output_yolo_dogcat\\val\n",
      "Create the dataset path file: C:\\Users\\USER\\Desktop\\Yolo-Fastest-darknet\\training_demo\\output_yolo_dogcat\\val.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#json_file_name = 'C:/Users/CYCHEN38/image_detection/Yolo-FastestV2/image_dataset/coco-2017-50-2class/raw/instances_train2017.json'\n",
    "custom_dataset = r'C:\\Users\\USER\\Desktop\\ML\\ML_tf2_object_detection_nu\\image_dataset\\coco-2017-50-2class'\n",
    "custom_dataset_output_yolo = r'C:\\Users\\USER\\Desktop\\Yolo-Fastest-darknet\\training_demo\\output_yolo_dogcat'\n",
    "category_class = ['cat','dog']\n",
    "#category_class = ['all']\n",
    "#kind = 'validation'\n",
    "#kind = 'train'\n",
    "\n",
    "def process_kind_path(kind, custom_dataset_output_yolo):\n",
    "    if kind == 'validation':\n",
    "        output_ann_path = os.path.join(custom_dataset_output_yolo, 'val')\n",
    "        output_dataset_path = os.path.join(custom_dataset_output_yolo, ('val.txt')) # dataset path\n",
    "    else:    \n",
    "        output_ann_path = os.path.join(custom_dataset_output_yolo, kind)                # yolo format annotation folder\n",
    "        output_dataset_path = os.path.join(custom_dataset_output_yolo, (kind + '.txt')) # dataset path\n",
    "    return output_ann_path, output_dataset_path\n",
    "        \n",
    "def create_coco_yolo_data(custom_dataset, custom_dataset_output_yolo, category_class, kind): \n",
    "\n",
    "    custom_dataset_folder = os.path.join(custom_dataset, kind)\n",
    "    \n",
    "    #json_file_name = os.path.join(now_path, custom_dataset, 'raw', 'instances_val2017.json') \n",
    "    json_file_name = os.path.join(custom_dataset_folder, 'labels.json') #Json file of COCO\n",
    "    imgs_folder = os.path.join(custom_dataset_folder, 'data')           #images folder\n",
    "    \n",
    "    output_ann_path, output_dataset_path = process_kind_path(kind, custom_dataset_output_yolo)\n",
    "    json_tf = json2txt.json2txt(json_file_name, custom_dataset_output_yolo, output_ann_path, category_class)\n",
    "    json_tf.run_annotation(imgs_folder, output_ann_path)\n",
    "    json_tf.create_dataset_path_txt(output_ann_path, output_dataset_path)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(custom_dataset_output_yolo, 'coco.names')):\n",
    "        json_tf.create_classes(os.path.join(custom_dataset_output_yolo, 'coco.names')) # create the label file\n",
    "    \n",
    "create_coco_yolo_data(custom_dataset, custom_dataset_output_yolo, category_class, 'train')\n",
    "create_coco_yolo_data(custom_dataset, custom_dataset_output_yolo, category_class, 'validation')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382410a-360c-4180-93a1-4edb863198bf",
   "metadata": {},
   "source": [
    "## 1. Custom dataset\n",
    "- If you want to create dataset path txt file, such as `train.txt` & `val.txt`, please use below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5739f84-6aaa-42f9-9892-7dd39a207b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create the dataset path file: C:\\Users\\USER\\Desktop\\ML\\ML_yolo\\medicine\\train.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json2txt\n",
    "import re\n",
    "\n",
    "new_imgs_folder = r\"C:\\Users\\USER\\Desktop\\ML\\ML_yolo\\medicine\\train\"\n",
    "output_dataset_path = r\"C:\\Users\\USER\\Desktop\\ML\\ML_yolo\\medicine\\train.txt\"\n",
    "#new_imgs_folder = r\"C:\\Users\\USER\\Desktop\\ML\\ML_yolo\\medicine\\val\"\n",
    "#output_dataset_path = r\"C:\\Users\\USER\\Desktop\\ML\\ML_yolo\\medicine\\val.txt\"\n",
    "\n",
    "def __load_images_from_folder(folder):  ### 使用yield\n",
    "        count = 0\n",
    "        for filename in os.listdir(folder):\n",
    "            count += 1\n",
    "            yield filename\n",
    "\n",
    "def create_dataset_path_txt(new_imgs_folder, output_dataset_path):\n",
    "        with open(f\"{output_dataset_path}\", \"a\") as file_object:\n",
    "            for img_filename in __load_images_from_folder(new_imgs_folder):\n",
    "                #if re.search('[0-9]*.jpg', img_filename):\n",
    "                if re.search(r'(?i)([a-zA-Z0-9\\s_\\\\.\\-\\(\\):])+(.jpg|.jpeg|.png)$', img_filename):\n",
    "                    file_object.write(f\"{os.path.join(new_imgs_folder, img_filename)}\\n\")\n",
    "        \n",
    "        print(\"Create the dataset path file: {}\".format(output_dataset_path))\n",
    "        \n",
    "create_dataset_path_txt(new_imgs_folder, output_dataset_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7d085-1830-46d7-90af-1bf096ea967a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
